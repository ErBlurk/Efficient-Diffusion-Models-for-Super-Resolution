{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7964d32",
   "metadata": {},
   "source": [
    "# Efficient Diffusion Models for Image Super-Resolution\n",
    "\n",
    "##### Neural Networks - Master in Artificial Intelligence and Robotics, Sapienza University of Rome\n",
    "\n",
    "---\n",
    "\n",
    "### Authors:\n",
    "> 1986191: Leonardo Mariut \\\n",
    "> 2190452: Mohamed Zakaria Benjelloun Tuimy\n",
    "\n",
    "---\n",
    "\n",
    "### Aim:\n",
    "\n",
    "This project improves ResShift for single-image super resolution, focusing on image quality while keeping model complexity and inference time comparable. Instead of directly predicting the full HR image, the network predicts residuals on top of a bicubic upsampled base. This residual formulation reduces redundancy, accelerates convergence, and lets the model specialize on recovering high-frequency details. The core idea is to learn features both in the spatial domain and in complementary frequency and wavelet domains, and to combine those feature so the network recovers fine textures and sharp edges efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### Papers:\n",
    "\n",
    "The work builds on a few recent papers and classic ideas. \n",
    "\n",
    "See Implicit Diffusion Models for Continuous Super-Resolution for diffusion conditioning ideas, arbitrary-steps Image Super-resolution via Diffusion Inversion for inversion and sampling strategies, \n",
    "\n",
    "Dual-domain Modulation Network for Lightweight Image Super-Resolution for guidance on fusing spatial and frequency streams, and NeRF for the intuition behind using frequency positional encodings. \n",
    "\n",
    "The links in the repo point to each paper:\n",
    "\n",
    "> [Implicit Diffusion Models for Continuous Super-Resolution](https://arxiv.org/abs/2303.16491) \\\n",
    "> [Arbitrary-steps Image Super-resolution via Diffusion Inversion](https://arxiv.org/html/2412.09013v1) \\\n",
    "> [Dual-domain Modulation Network for Lightweight Image Super-Resolution](https://arxiv.org/abs/2503.10047) \n",
    "\n",
    "---\n",
    "\n",
    "### Key concepts:\n",
    "\n",
    "**Fourier transform**: Fourier transforms decompose image content into frequency components so amplitude and phase information becomes explicit. High frequency coefficients capture edges and fine texture, low frequency coefficients capture coarse structure. Working with frequency representations lets the model pay direct attention to texture and edge fidelity that spatial convolutions can miss.\n",
    "\n",
    "**Conditioned diffusion model**: The model is a conditional reverse Markov process that starts from a noisy initial state informed by the bicubic upsampled low resolution image. Conditioning on that base image gives a much faster and more stable inference path than starting from pure noise, and it biases the sampler to preserve global structure while refining high frequency detail. The diffusion model predicts residuals relative to the bicubic input rather than the absolute HR image.\n",
    "\n",
    "**Laplacian pyramids**: A Laplacian pyramid is a multi-scale image representation where an image is decomposed into progressively lower-resolution approximations and the band-pass residuals between them. For super-resolution, this means the network can reconstruct fine details in a coarse-to-fine manner: at each level, the model predicts residuals that correct and refine the upsampled lower-resolution estimate.\n",
    "\n",
    "**UNet backbone**: The network uses a standard encoder-decoder topology with skip connections. Time conditioning is injected via a sinusoidal positional embedding fed through a small MLP so the network knows the current reverse step. The UNet is a convenient backbone because it mixes local and contextual information at multiple scales while keeping spatial resolution alignment for skip connections.\n",
    "\n",
    "**Dual-domain fusion**: The model processes features in three parallel branches: a spatial branch for standard convolutional processing, a DCT branch that explicitly filters and learns in the frequency domain, and a DWT branch that captures multi-resolution wavelet coefficients. Outputs from the three branches are concatenated and fused, letting the network integrate texture and structure cues from complementary representations.\n",
    "\n",
    "**Eta schedule**: The noise schedule eta(t) controls how the noisy mixture between x0 and y0 evolves over the forward process. We parametrize a smoothly decaying schedule that keeps early steps relatively noisy while letting later steps converge, which stabilizes training and produces better sampling behavior when reversing the chain.\n",
    "\n",
    "**Perceptual loss**: An optional perceptual loss uses VGG features to compare predicted and ground truth images in feature space rather than only pixel space. Perceptual loss helps with texture realism and avoids some of the blurring effects that pure L1 or L2 losses introduce, while still being lightweight when used with a small weight.\n",
    "\n",
    "**Evaluation metrics**: We report PSNR and SSIM on the validation set. PSNR measures mean squared error in log scale and captures overall pixel fidelity. SSIM measures structural similarity and better correlates with perceived image quality for small structural differences. Both metrics are useful together because PSNR penalizes global error while SSIM focuses on local structural preservation.\n",
    "\n",
    "**Frequency positional encodings**: Inspired by NeRF, we add Fourier-style positional encodings to some inputs so the network has access to a richer set of sinusoidal basis functions of different frequencies. This gives the model an easy, spatially aware mechanism to represent high frequency variations and to modulate predictions according to local spatial phase, which is helpful when reconstructing textures and fine edges at high upsampling factors. When used with Laplacian pyramids, frequency encodings can be applied at each pyramid level with scale-dependent frequency ranges. This ensures residuals at finer pyramid levels receive richer high-frequency modulation, which is critical for textures like grass, hair, or fabric.\n",
    "\n",
    "---\n",
    "\n",
    "### The dataset:\n",
    "\n",
    "We use [DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K) as the primary dataset. The dataset was downloaded from the DIV2K distribution and contains 800 high resolution images for training and 200 high resolution images for validation in our setup. Training images are cropped into random patches to build the training samples, while validation uses original HR patches so metrics are measured on true full-resolution content. For the baseline comparison the low resolution input is produced by lossy bicubic downsampling followed by bicubic upsampling to the target size (4x), which also serves as the y0 base image the model conditions on.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e0b1b",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4fac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/leo/anaconda3/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /home/leo/anaconda3/lib/python3.12/site-packages (0.23.0)\n",
      "Requirement already satisfied: torch_dct in /home/leo/anaconda3/lib/python3.12/site-packages (0.1.6)\n",
      "Requirement already satisfied: pytorch-wavelets in /home/leo/anaconda3/lib/python3.12/site-packages (1.3.0)\n",
      "Requirement already satisfied: filelock in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/leo/anaconda3/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: numpy in /home/leo/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/leo/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: six in /home/leo/anaconda3/lib/python3.12/site-packages (from pytorch-wavelets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/leo/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/leo/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torch_dct pytorch-wavelets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef4af4",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aff1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, re\n",
    "from glob import glob\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "\n",
    "# external packages required: \n",
    "from torch_dct import dct_2d, idct_2d\n",
    "from pytorch_wavelets import DWTForward, DWTInverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd57d6e",
   "metadata": {},
   "source": [
    "## 2. Globals and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# flags, paths, model and training hyperparams\n",
    "DEBUG_RUN = False\n",
    "\n",
    "# Dataset\n",
    "DATA_ROOT = \"datasets/DIV2K\"\n",
    "TRAIN_HR_DIR = os.path.join(DATA_ROOT, \"train/HR\")\n",
    "VALID_HR_DIR = os.path.join(DATA_ROOT, \"valid/HR\")\n",
    "SAVE_DIR = \"checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Markov chain and network\n",
    "SCALE = 4\n",
    "T_STEPS = 15\n",
    "KAPPA = 2.0\n",
    "ETA_1_FIXED = 0.008\n",
    "P_SCHEDULE = 0.3\n",
    "NUM_CHANNELS = 64 # UNet channels\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 380\n",
    "FREQ_LOSS_WEIGHT = 2.0\n",
    "SAVE_INTERVAL = 10 # Epochs save weights interval\n",
    "EVAL_INTERVAL = 10 # Epochs evaluate interval\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 144\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Additional setup - positional encodings and perceptual loss\n",
    "USE_FOURIER = True\n",
    "NUM_POS_FREQ = 6\n",
    "USE_AMPLITUDE = True\n",
    "AMP_KERNEL = 7\n",
    "MULTISCALE_SCALES = (1, 2, 4)\n",
    "\n",
    "USE_VGG_PERCEPTUAL = False\n",
    "PERCEPTUAL_WEIGHT = 0.05\n",
    "\n",
    "POS_CHANNELS = (4 * NUM_POS_FREQ) if USE_FOURIER else 0\n",
    "AMP_CHANNELS = 1 if USE_AMPLITUDE else 0\n",
    "BASE_IN_CHANNELS = 6 + POS_CHANNELS + AMP_CHANNELS\n",
    "\n",
    "# Run few epochs mostly to test code\n",
    "if DEBUG_RUN:\n",
    "    print(\"DEBUG RUN: small dataset/epochs for quick test\")\n",
    "    NUM_EPOCHS = 2\n",
    "    BATCH_SIZE = 4\n",
    "    EVAL_INTERVAL = 1\n",
    "    SAVE_INTERVAL = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501037c",
   "metadata": {},
   "source": [
    "## 3. Utils\n",
    "\n",
    "Positional encodings and amplitude helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f953972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier positional encoding and local amplitude map\n",
    "def get_2d_fourier_pos_enc(H: int, W: int, n_freqs: int = NUM_POS_FREQ, base: float = 2.0, device=None) -> torch.Tensor:\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ys = torch.linspace(0.0, 1.0, H, device=device)\n",
    "    xs = torch.linspace(0.0, 1.0, W, device=device)\n",
    "    grid_y, grid_x = torch.meshgrid(ys, xs, indexing='ij')  # [H,W]\n",
    "    coords = torch.stack([grid_x, grid_y], dim=-1)           # [H,W,2]\n",
    "    freqs = (base ** torch.arange(n_freqs, device=device)) * math.pi\n",
    "    encs = []\n",
    "    for f in freqs:\n",
    "        encs.append(torch.sin(coords * f))\n",
    "        encs.append(torch.cos(coords * f))\n",
    "    enc = torch.cat(encs, dim=-1)   # [H,W,4*n_freqs]\n",
    "    return enc.permute(2, 0, 1).unsqueeze(0)  # [1,C,H,W]\n",
    "\n",
    "\n",
    "def compute_local_amplitude(x: torch.Tensor, kernel_size: int = AMP_KERNEL, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # x in [0,1], returns [B,1,H,W] normalized amplitude map\n",
    "    mu = F.avg_pool2d(x, kernel_size, stride=1, padding=kernel_size//2)\n",
    "    mu2 = F.avg_pool2d(x * x, kernel_size, stride=1, padding=kernel_size//2)\n",
    "    var = (mu2 - mu * mu).clamp(min=0.0)\n",
    "    amp = torch.sqrt(var.mean(dim=1, keepdim=True) + eps)\n",
    "    B = amp.shape[0]\n",
    "    flat = amp.view(B, -1)\n",
    "    amin = flat.min(dim=1)[0].view(B,1,1,1)\n",
    "    amax = flat.max(dim=1)[0].view(B,1,1,1)\n",
    "    amp = (amp - amin) / (amax - amin + eps)\n",
    "    return amp\n",
    "\n",
    "\n",
    "def multiscale_loss(pred: torch.Tensor, target: torch.Tensor, scales=MULTISCALE_SCALES, spatial_loss_fn=None, freq_weight: float = FREQ_LOSS_WEIGHT, spatial_weight: float = 1.0) -> torch.Tensor:\n",
    "    # combined L1 spatial + DCT L1 across scales, expects pred/target in [-1,1]\n",
    "    if spatial_loss_fn is None:\n",
    "        spatial_loss_fn = nn.L1Loss()\n",
    "    total = 0.0\n",
    "    total_weight = 0.0\n",
    "    pred01 = (pred + 1.0) / 2.0\n",
    "    target01 = (target + 1.0) / 2.0\n",
    "    for s in scales:\n",
    "        if s == 1:\n",
    "            p_s, t_s = pred01, target01\n",
    "        else:\n",
    "            p_s = F.interpolate(pred01, scale_factor=1.0/s, mode='bilinear', align_corners=False)\n",
    "            t_s = F.interpolate(target01, scale_factor=1.0/s, mode='bilinear', align_corners=False)\n",
    "        l_spatial = spatial_loss_fn(p_s, t_s)\n",
    "        p_dct = dct_2d(p_s, norm='ortho')\n",
    "        t_dct = dct_2d(t_s, norm='ortho')\n",
    "        l_freq = spatial_loss_fn(p_dct, t_dct)\n",
    "        weight = 1.0 / float(s)\n",
    "        total += weight * (spatial_weight * l_spatial + freq_weight * l_freq)\n",
    "        total_weight += weight\n",
    "    return total / (total_weight + 1e-12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6ca0b",
   "metadata": {},
   "source": [
    "VGG perceptual loss helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860b7ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VGG features if requested\n",
    "def load_vgg_features(model_name: str = \"vgg19_bn\", pretrained: bool = True, device: torch.device = torch.device(\"cpu\")):\n",
    "    try:\n",
    "        if model_name == \"vgg19_bn\":\n",
    "            from torchvision.models import vgg19_bn, VGG19_BN_Weights\n",
    "            weights = VGG19_BN_Weights.DEFAULT if pretrained else None\n",
    "            model = vgg19_bn(weights=weights).features.to(device).eval()\n",
    "        else:\n",
    "            from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "            weights = VGG16_BN_Weights.DEFAULT if pretrained else None\n",
    "            model = vgg16_bn(weights=weights).features.to(device).eval()\n",
    "    except Exception:\n",
    "        if model_name == \"vgg19_bn\":\n",
    "            from torchvision.models import vgg19_bn\n",
    "            model = vgg19_bn(pretrained=pretrained).features.to(device).eval()\n",
    "        else:\n",
    "            from torchvision.models import vgg16_bn\n",
    "            model = vgg16_bn(pretrained=pretrained).features.to(device).eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    return model\n",
    "\n",
    "\n",
    "if USE_VGG_PERCEPTUAL:\n",
    "    vgg = load_vgg_features(model_name=\"vgg19_bn\", pretrained=True, device=DEVICE)\n",
    "    def vgg_features(x):\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406], device=x.device).view(1,3,1,1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225], device=x.device).view(1,3,1,1)\n",
    "        x_norm = (x - mean) / std\n",
    "        return vgg(x_norm)\n",
    "    def perceptual_loss(pred, target):\n",
    "        p = (pred + 1.0) / 2.0\n",
    "        t = (target + 1.0) / 2.0\n",
    "        return F.mse_loss(vgg_features(p), vgg_features(t))\n",
    "else:\n",
    "    def perceptual_loss(pred, target): return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908336c",
   "metadata": {},
   "source": [
    "## 4. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIV2K dataset that returns HR and upsampled LR (y0)\n",
    "class DIV2KDataset(Dataset):\n",
    "    def __init__(self, hr_dir: str, crop_size: int = 256, scale: int = SCALE):\n",
    "        self.paths = sorted(glob(os.path.join(hr_dir, \"*.png\")))\n",
    "        self.crop_size = crop_size\n",
    "        self.scale = scale\n",
    "        self.transform = T.Compose([T.ToTensor(), T.Normalize(mean=[0.5]*3, std=[0.5]*3)])  # [0,1] -> [-1,1]\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        hr_image = Image.open(self.paths[idx]).convert('RGB')\n",
    "        w,h = hr_image.size\n",
    "        if w >= self.crop_size and h >= self.crop_size:\n",
    "            x = random.randint(0, w - self.crop_size)\n",
    "            y = random.randint(0, h - self.crop_size)\n",
    "            hr_patch = hr_image.crop((x,y,x+self.crop_size,y+self.crop_size))\n",
    "        else:\n",
    "            hr_patch = hr_image.resize((self.crop_size,self.crop_size), Image.BICUBIC)\n",
    "        lr_w, lr_h = self.crop_size // self.scale, self.crop_size // self.scale\n",
    "        lr_patch = hr_patch.resize((lr_w, lr_h), Image.BICUBIC)\n",
    "        y0_patch = lr_patch.resize((self.crop_size, self.crop_size), Image.BICUBIC)\n",
    "        hr_tensor = self.transform(hr_patch)\n",
    "        y0_tensor = self.transform(y0_patch)\n",
    "        return hr_tensor, y0_tensor\n",
    "\n",
    "\n",
    "# Dataloaders and positional encoding cache\n",
    "train_dataset = DIV2KDataset(TRAIN_HR_DIR, crop_size=256, scale=SCALE)\n",
    "valid_dataset = DIV2KDataset(VALID_HR_DIR, crop_size=256, scale=SCALE)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "if DEBUG_RUN:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "POS_ENC_CACHE = None\n",
    "def ensure_pos_enc(H: int, W: int):\n",
    "    global POS_ENC_CACHE\n",
    "    if POS_ENC_CACHE is None or POS_ENC_CACHE.shape[-2:] != (H,W):\n",
    "        POS_ENC_CACHE = get_2d_fourier_pos_enc(H, W, n_freqs=NUM_POS_FREQ, device=DEVICE)\n",
    "    return POS_ENC_CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39161587",
   "metadata": {},
   "source": [
    "## 5. Network\n",
    "\n",
    "Simple UNet with Spatial and Freqeuncy/Wavelet domain blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77822910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network blocks\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim:int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        freqs = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        vals = t[:, None] * freqs[None, :]\n",
    "        return torch.cat((vals.sin(), vals.cos()), dim=-1)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels:int, out_channels:int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "\n",
    "class DCTBranch(nn.Module):\n",
    "    def __init__(self, channels:int):\n",
    "        super().__init__()\n",
    "        self.process = nn.Sequential(nn.Conv2d(channels, channels, 1), nn.ReLU(inplace=True), nn.Conv2d(channels, channels, 1))\n",
    "    def forward(self, x):\n",
    "        x_dct = dct_2d(x, norm='ortho')\n",
    "        x_dct = self.process(x_dct)\n",
    "        return idct_2d(x_dct, norm='ortho')\n",
    "\n",
    "\n",
    "class DWTBranch(nn.Module):\n",
    "    def __init__(self, channels:int):\n",
    "        super().__init__()\n",
    "        self.dwt = DWTForward(J=1, wave='haar', mode='reflect')\n",
    "        self.idwt = DWTInverse(wave='haar', mode='reflect')\n",
    "        self.ll_process = nn.Conv2d(channels, channels, 1)\n",
    "        self.lh_process = nn.Conv2d(channels, channels, 1)\n",
    "        self.hl_process = nn.Conv2d(channels, channels, 1)\n",
    "        self.hh_process = nn.Conv2d(channels, channels, 1)\n",
    "    def forward(self, x):\n",
    "        yl, yh = self.dwt(x)\n",
    "        yl = self.ll_process(yl)\n",
    "        lh_proc = self.lh_process(yh[0][:, :, 0, :, :])\n",
    "        hl_proc = self.hl_process(yh[0][:, :, 1, :, :])\n",
    "        hh_proc = self.hh_process(yh[0][:, :, 2, :, :])\n",
    "        yh[0] = torch.stack([lh_proc, hl_proc, hh_proc], dim=2)\n",
    "        return self.idwt((yl, yh))\n",
    "\n",
    "\n",
    "class DualDomainBlock(nn.Module):\n",
    "    def __init__(self, channels:int):\n",
    "        super().__init__()\n",
    "        self.spatial_branch = ConvBlock(channels, channels)\n",
    "        self.dct_branch = DCTBranch(channels)\n",
    "        self.dwt_branch = DWTBranch(channels)\n",
    "        self.fusion = nn.Conv2d(channels * 3, channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        x_spatial = self.spatial_branch(x)\n",
    "        x_dct = self.dct_branch(x)\n",
    "        x_dwt = self.dwt_branch(x)\n",
    "        combined = torch.cat([x_spatial, x_dct, x_dwt], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        return x + fused\n",
    "\n",
    "\n",
    "# UNet with time conditioning\n",
    "class DualDomainUNet(nn.Module):\n",
    "    def __init__(self, in_channels:int = BASE_IN_CHANNELS, base_channels:int = NUM_CHANNELS, time_dim:int = 128):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(SinusoidalPosEmb(time_dim), nn.Linear(time_dim, time_dim * 4), nn.GELU(), nn.Linear(time_dim * 4, base_channels))\n",
    "        self.inc = ConvBlock(in_channels, base_channels)\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DualDomainBlock(base_channels))\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DualDomainBlock(base_channels))\n",
    "        self.bot = DualDomainBlock(base_channels)\n",
    "        self.up1 = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), nn.Conv2d(base_channels, base_channels, 1))\n",
    "        self.dec1 = DualDomainBlock(base_channels * 2)\n",
    "        self.up2 = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True), nn.Conv2d(base_channels * 2, base_channels, 1))\n",
    "        self.dec2 = DualDomainBlock(base_channels * 2)\n",
    "        self.outc = nn.Conv2d(base_channels * 2, 3, kernel_size=1)\n",
    "    def forward(self, xt: torch.Tensor, y0: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        if t.dim() > 1: t = t.squeeze(1)\n",
    "        t = t.to(xt.device).float()\n",
    "        t_emb = self.time_mlp(t).unsqueeze(-1).unsqueeze(-1)\n",
    "        x_in = torch.cat([xt, y0], dim=1)\n",
    "        x1 = self.inc(x_in)\n",
    "        x1 = x1 + t_emb\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x_bot = self.bot(x3)\n",
    "        x = self.up1(x_bot)\n",
    "        x = self.dec1(torch.cat([x, x2], dim=1))\n",
    "        x = self.up2(x)\n",
    "        x = self.dec2(torch.cat([x, x1], dim=1))\n",
    "        return self.outc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d29de26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise schedule\n",
    "def make_eta_schedule(T: int, p: float = P_SCHEDULE, kappa: float = KAPPA) -> np.ndarray:\n",
    "    eta = np.zeros(T + 1, dtype=np.float32)\n",
    "    eta[1] = ETA_1_FIXED\n",
    "    b0 = np.exp(0.5 / (T - 1) * np.log(0.999 / eta[1]))\n",
    "    for t in range(2, T + 1):\n",
    "        exponent = ((t-1) / (T-1)) ** p\n",
    "        eta[t] = (np.sqrt(eta[1]) * (b0 ** ((T - 1) * exponent)))**2\n",
    "    return eta\n",
    "eta = torch.from_numpy(make_eta_schedule(T_STEPS, p=P_SCHEDULE, kappa=KAPPA)).to(DEVICE)\n",
    "alpha = torch.diff(eta, prepend=torch.tensor([0.0], device=DEVICE))\n",
    "\n",
    "\n",
    "# Augment y0\n",
    "def augment_y0(y0: torch.Tensor) -> torch.Tensor:\n",
    "    B,C,H,W = y0.shape\n",
    "    extras = []\n",
    "    if USE_FOURIER:\n",
    "        pos = ensure_pos_enc(H, W).to(y0.device)\n",
    "        extras.append(pos.repeat(B,1,1,1))\n",
    "    if USE_AMPLITUDE:\n",
    "        y0_01 = (y0 + 1.0) / 2.0\n",
    "        amp = compute_local_amplitude(y0_01, kernel_size=AMP_KERNEL)\n",
    "        extras.append(amp.to(y0.device))\n",
    "    return torch.cat([y0] + extras, dim=1) if extras else y0\n",
    "\n",
    "\n",
    "# Sampling helpers\n",
    "def sample_xt(x0: torch.Tensor, y0: torch.Tensor, t_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    eta_t = eta[t_idx]\n",
    "    mean = (1 - eta_t) * x0 + eta_t * y0\n",
    "    noise = torch.randn_like(x0)\n",
    "    xt = mean + KAPPA * torch.sqrt(eta_t) * noise\n",
    "    return xt, noise\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def reverse_sample_augmented(y0: torch.Tensor) -> Tuple[torch.Tensor, list]:\n",
    "    model.eval()\n",
    "    chain_steps = []\n",
    "    x_t = y0 + KAPPA * torch.sqrt(eta[T_STEPS]) * torch.randn_like(y0)\n",
    "    chain_steps.append(x_t)\n",
    "\n",
    "    for t in range(T_STEPS, 0, -1):\n",
    "        t_tensor = torch.full((y0.shape[0],), float(t), device=DEVICE)\n",
    "        y0_aug = augment_y0(y0)\n",
    "        predicted_residual = model(x_t, y0_aug, t_tensor)\n",
    "        predicted_x0 = (y0 + predicted_residual).clamp(-1.0, 1.0)  # learn residuals\n",
    "        eta_t, eta_t_1, alpha_t = eta[t], eta[t-1], alpha[t]\n",
    "        term1 = (eta_t_1 / eta_t) * x_t\n",
    "        term2 = (alpha_t / eta_t) * (1 - eta_t_1) * predicted_x0\n",
    "        term3 = (alpha_t * eta_t_1 / eta_t) * y0\n",
    "        mu = term1 + term2 + term3\n",
    "        variance = (KAPPA ** 2) * (eta_t_1 / eta_t) * alpha_t\n",
    "        x_t = mu + torch.sqrt(variance) * torch.randn_like(x_t) if t > 1 else mu\n",
    "        if t == (T_STEPS // 2) + 1:\n",
    "            chain_steps.append(x_t)\n",
    "\n",
    "    final_sr = (x_t.clamp(-1,1) + 1) / 2.0\n",
    "    processed_chain = [(step.clamp(-1,1) + 1)/2.0 for step in chain_steps]\n",
    "    return final_sr, processed_chain\n",
    "\n",
    "# Evaluation and plotting\n",
    "@torch.no_grad()\n",
    "def evaluate_and_plot(model, valid_loader, epoch, n_examples=3):\n",
    "    model.eval()\n",
    "    total_psnr_sr, total_psnr_bic, total_ssim = 0.0, 0.0, 0.0\n",
    "    plot_data = []\n",
    "\n",
    "    for i, (hr_img, y0_img) in enumerate(valid_loader):\n",
    "        hr, y0 = hr_img.to(DEVICE), y0_img.to(DEVICE)\n",
    "        sr_tensor, _ = reverse_sample_augmented(y0)\n",
    "\n",
    "        hr_np = (hr.squeeze(0).cpu().permute(1,2,0).numpy() + 1)/2.0\n",
    "        y0_np = (y0.squeeze(0).cpu().permute(1,2,0).numpy() + 1)/2.0\n",
    "        sr_np = sr_tensor.squeeze(0).cpu().permute(1,2,0).numpy()\n",
    "\n",
    "        psnr_sr = compare_psnr(hr_np, sr_np, data_range=1.0)\n",
    "        psnr_bic = compare_psnr(hr_np, y0_np, data_range=1.0)\n",
    "\n",
    "        win_size = min(7, min(hr_np.shape[:2]))\n",
    "        ssim = compare_ssim(hr_np, sr_np, data_range=1.0, channel_axis=2, win_size=win_size)\n",
    "\n",
    "        total_psnr_sr += psnr_sr\n",
    "        total_psnr_bic += psnr_bic\n",
    "        total_ssim += ssim\n",
    "\n",
    "        if i < n_examples:\n",
    "            plot_data.append({'hr':hr_np, 'lr':y0_np, 'sr':sr_np, 'psnr_sr':psnr_sr, 'ssim':ssim, 'psnr_bic':psnr_bic})\n",
    "\n",
    "    n = len(valid_loader)\n",
    "    print(f\"Eval Epoch {epoch}: PSNR(SR)={total_psnr_sr/n:.4f}, PSNR(Bic)={total_psnr_bic/n:.4f}, SSIM={total_ssim/n:.4f}\")\n",
    "\n",
    "    fig, axes = plt.subplots(len(plot_data), 3, figsize=(15,5*len(plot_data)))\n",
    "    if len(plot_data) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, d in enumerate(plot_data):\n",
    "        ax_row = axes[i]\n",
    "        ax_row[0].imshow(d['lr']); ax_row[0].set_title(\"LR (bic) {:.2f} dB\".format(d[\"psnr_bic\"]))\n",
    "        ax_row[1].imshow(d['sr']); ax_row[1].set_title(\"SR (final) PSNR: {:.2f} dB, SSIM: {:.4f}\".format(d[\"psnr_sr\"], d[\"ssim\"]))\n",
    "        ax_row[2].imshow(d['hr']); ax_row[2].set_title(\"HR (Ground Truth)\")\n",
    "        for ax in ax_row:\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(SAVE_DIR, \"eval_epoch_{}.png\".format(epoch))\n",
    "    plt.savefig(out_path); plt.close()\n",
    "    print(\"Saved eval plot to\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70fa05",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492462f1",
   "metadata": {},
   "source": [
    "Instantiate model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "172fa905",
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = DualDomainUNet(in_channels=BASE_IN_CHANNELS, base_channels=NUM_CHANNELS).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7ea29",
   "metadata": {},
   "source": [
    "Ceckpoint loading (from last saved weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff54e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints/resshift_epoch_370.pt'. Resuming at epoch 371.\n"
     ]
    }
   ],
   "source": [
    "RESUME_FROM = None  # or set to path to resume - wieghts name\n",
    "\n",
    "def save_checkpoint(epoch: int):\n",
    "    ck = os.path.join(SAVE_DIR, \"resshift_epoch_{}.pt\".format(epoch))\n",
    "    torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'scheduler_state': scheduler.state_dict()}, ck)\n",
    "    print(\"Saved checkpoint:\", ck)\n",
    "\n",
    "\n",
    "def find_latest_checkpoint() -> str | None:\n",
    "    candidates = glob(os.path.join(SAVE_DIR, \"resshift_epoch_*.pt\"))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    best = None\n",
    "    best_epoch = -1\n",
    "    for p in candidates:\n",
    "        m = re.search(r\"resshift_epoch_(\\d+)\\.pt$\", p)\n",
    "        if m:\n",
    "            e = int(m.group(1))\n",
    "            if e > best_epoch:\n",
    "                best_epoch = e\n",
    "                best = p\n",
    "    return best\n",
    "\n",
    "\n",
    "def _move_optimizer_state_to_device(opt_state, device):\n",
    "    # move optimizer state tensors to device\n",
    "    for state in list(opt_state.values()):\n",
    "        for k, v in list(state.items()):\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "\n",
    "\n",
    "def load_checkpoint(path: str):\n",
    "    ck = torch.load(path, map_location=DEVICE)\n",
    "    mfile = re.search(r\"resshift_epoch_(\\d+)\\.pt$\", path)\n",
    "    epoch_from_filename = int(mfile.group(1)) if mfile else None\n",
    "\n",
    "    start_epoch = 1\n",
    "    if isinstance(ck, dict):\n",
    "        model_state = None\n",
    "        if 'model_state' in ck:\n",
    "            model_state = ck['model_state']\n",
    "        elif 'model_state_dict' in ck:\n",
    "            model_state = ck['model_state_dict']\n",
    "        elif 'state_dict' in ck:\n",
    "            model_state = ck['state_dict']\n",
    "\n",
    "        if model_state is not None:\n",
    "            model.load_state_dict(model_state)\n",
    "            if 'optimizer_state' in ck:\n",
    "                try:\n",
    "                    optimizer.load_state_dict(ck['optimizer_state'])\n",
    "                    _move_optimizer_state_to_device(optimizer.state, DEVICE)\n",
    "                except Exception as e:\n",
    "                    print(\"Warning: couldn't load optimizer_state cleanly:\", e)\n",
    "            if 'scheduler_state' in ck:\n",
    "                try:\n",
    "                    scheduler.load_state_dict(ck['scheduler_state'])\n",
    "                except Exception as e:\n",
    "                    print(\"Warning: couldn't load scheduler_state cleanly:\", e)\n",
    "            if 'epoch' in ck:\n",
    "                start_epoch = int(ck.get('epoch', 0)) + 1\n",
    "            elif epoch_from_filename is not None:\n",
    "                start_epoch = epoch_from_filename + 1\n",
    "            else:\n",
    "                start_epoch = 1\n",
    "        else:\n",
    "            # maybe ck is a raw state_dict\n",
    "            try:\n",
    "                model.load_state_dict(ck)\n",
    "                start_epoch = epoch_from_filename + 1 if epoch_from_filename is not None else 1\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Unrecognized checkpoint format for '{path}': {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            model.load_state_dict(ck)\n",
    "            start_epoch = epoch_from_filename + 1 if epoch_from_filename is not None else 1\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Unrecognized checkpoint format for '{path}': {e}\")\n",
    "\n",
    "    scheduler.last_epoch = max(0, start_epoch - 1)\n",
    "    print(f\"Loaded checkpoint '{path}'. Resuming at epoch {start_epoch}.\")\n",
    "    return start_epoch\n",
    "\n",
    "\n",
    "start_epoch = 1\n",
    "latest = RESUME_FROM if RESUME_FROM else find_latest_checkpoint()\n",
    "if latest is not None:\n",
    "    try:\n",
    "        start_epoch = load_checkpoint(latest)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load checkpoint:\", e)\n",
    "        print(\"Starting from scratch.\")\n",
    "        start_epoch = 1\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551d2ec",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eab4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    print(\"Starting training...\")\n",
    "    global start_epoch\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "        for batch_idx, (x0_batch, y0_batch) in enumerate(pbar):\n",
    "            x0, y0 = x0_batch.to(DEVICE), y0_batch.to(DEVICE)\n",
    "            t_indices = torch.randint(1, T_STEPS + 1, (x0.shape[0],), device=DEVICE)\n",
    "\n",
    "            xt_list, noise_list = [], []\n",
    "            for i, t in enumerate(t_indices):\n",
    "                xt, noise = sample_xt(x0[i:i+1], y0[i:i+1], t.item())\n",
    "                xt_list.append(xt)\n",
    "                noise_list.append(noise)\n",
    "            xt = torch.cat(xt_list, dim=0)\n",
    "\n",
    "            y0_aug = augment_y0(y0)\n",
    "\n",
    "            # model predicts residual (x0 - y0)\n",
    "            predicted_residual = model(xt, y0_aug, t_indices.float())\n",
    "            predicted_x0 = y0 + predicted_residual\n",
    "\n",
    "            loss_ms = multiscale_loss(predicted_x0, x0, scales=MULTISCALE_SCALES, spatial_loss_fn=nn.L1Loss(), freq_weight=FREQ_LOSS_WEIGHT)\n",
    "            loss_perc = PERCEPTUAL_WEIGHT * perceptual_loss(predicted_x0, x0) if USE_VGG_PERCEPTUAL else 0.0\n",
    "            loss = loss_ms + loss_perc\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch} done. Avg loss: {avg_loss:.5f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % EVAL_INTERVAL == 0 and epoch > 0:\n",
    "            evaluate_and_plot(model, valid_dataloader, epoch)\n",
    "\n",
    "        if epoch % SAVE_INTERVAL == 0 and epoch > 0:\n",
    "            save_checkpoint(epoch)\n",
    "\n",
    "    final_ck = os.path.join(SAVE_DIR, \"resshift_final.pt\")\n",
    "    torch.save({\n",
    "        'epoch': NUM_EPOCHS,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'scheduler_state': scheduler.state_dict(),\n",
    "    }, final_ck)\n",
    "    print(\"Training finished. Final model saved to\", final_ck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0007d",
   "metadata": {},
   "source": [
    "## 7. Run training and evaluate progressively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de88ad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready. Model summary:\n",
      "Forward pass OK. Output shape: torch.Size([1, 3, 256, 256])\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f92028dac64614a4c682fe9fdb1781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 371/380:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371 done. Avg loss: 0.06581\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3d621170844b42a03b168a05ac2e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 372/380:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 372 done. Avg loss: 0.06866\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4b0f06df534e50b8e1c37e146b7b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 373/380:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373 done. Avg loss: 0.06735\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f4af58c512451caf6ed915899b9c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 374/380:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 done. Avg loss: 0.06663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9705d12a844f9b840e0ba0531b4d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 375/380:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375 done. Avg loss: 0.06684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad97d57d987463798c1b0598ad92383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 376/380:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376 done. Avg loss: 0.06781\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42138f70cbe840f4a6a1064bc310fd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 377/380:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 377 done. Avg loss: 0.06705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3e6f142bd644d6b057ccca128ba2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 378/380:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 378 done. Avg loss: 0.06889\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915434be50d545d6b99c7e9fe62f44eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 379/380:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379 done. Avg loss: 0.06805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228521feb1b54dd5949df1320ea66bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 380/380:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380 done. Avg loss: 0.06918\n",
      "Eval Epoch 380: PSNR(SR)=28.7265, PSNR(Bic)=28.1391, SSIM=0.7826\n",
      "Saved eval plot to checkpoints/eval_epoch_380.png\n",
      "Saved checkpoint: checkpoints/resshift_epoch_380.pt\n",
      "Training finished. Final model saved to checkpoints/resshift_final.pt\n"
     ]
    }
   ],
   "source": [
    "print(\"Ready. Model summary:\")\n",
    "with torch.no_grad():\n",
    "    xt_test = torch.randn(1,3,256,256, device=DEVICE)\n",
    "    y0_test = torch.randn(1,3,256,256, device=DEVICE)\n",
    "    ensure_pos_enc(256, 256)\n",
    "    y0_aug_test = augment_y0(y0_test)\n",
    "    t_test = torch.tensor([1.0], device=DEVICE)\n",
    "    out = model(xt_test, y0_aug_test, t_test)\n",
    "    print(\"Forward pass OK. Output shape:\", out.shape)\n",
    "\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f16a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Limitations and reflections\n",
    "\n",
    "1. Early training often shows color shifts or wrong channel mapping during the first few epochs. This typically results from inconsistent normalization or from accidentally changing channel order between data loading, augmentation and model input.\n",
    "\n",
    "2. Checkerboard artifacts and ringing appear when high frequency terms are learned aggressively or when upsampling is implemented with transposed convolutions.\n",
    "\n",
    "3. Training stability is a recurring problem. runs have shown exploding gradients in early stages and stalled or vanishing gradients in later stages where the model appears not to learn.\n",
    "\n",
    "4. The plain L1 objective can bias the model to reproduce the bicubic upsample because that solution is already close in pixel space.\n",
    "\n",
    "5. Dataset composition is a limiting factor. DIV2K contains many low-texture or blurred regions that make selective sharpening difficult and that increase variance in evaluation.\n",
    "\n",
    "6. Memory, batch size and throughput are practical constraints on a 16 GB GPU. small batches limit optimizer behavior and reduce the number of unique samples seen per epoch.\n",
    "\n",
    "7. Matching spatial and frequency or wavelet features is difficult in practice. fused features may conflict and the network can favor one domain over the other, causing inconsistent restorations.\n",
    "\n",
    "8. Artifacts observed include jagged edges, blurred textures, edge ringing and mismatched textures in uniform-color regions.\n",
    "\n",
    "9. The eta schedule for the Markov chain and the kappa coefficient are sensitive hyperparameters. small changes can noticeably alter sampling dynamics and final outputs.\n",
    "\n",
    "10. The model capacity chosen here (base channels = 64) is small and can struggle to generalize to complex textures while still being prone to underfitting in some regions.\n",
    "\n",
    "11. Small differences between training and evaluation implementations can produce large gaps in results. common mismatches include normalization and channel order, different up and down sampling kernels, inconsistent padding modes for convolutions and wavelet transforms, running networks in train mode at evaluation time, and precision differences between mixed precision training and full precision evaluation.\n",
    "\n",
    "12. Paper reproducibility often fails on minor implementation details that are not explicitly stated. Other difficulties include implemented the stated details.\n",
    "\n",
    "13. PSNR and SSIM do not always correlate with perceived image quality. PSNR measures mean squared error in log scale and SSIM measures local structural similarity, but both can favor overly smooth outputs that lack realistic high frequency detail; perceptual quality can degrade even when these metrics improve. Good examples are good SR images getting lower scores compared to their LR bicubic upsampled equivalent. \n",
    "\n",
    "14. Final super-resolved images sometimes remain blurry or exhibit jagged edges despite reasonable numeric metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical fixes and experiment ideas\n",
    "\n",
    "1. Implement a training schedule that starts with only L1 for the first N epochs, then gradually adds perceptual and frequency losses. \n",
    "\n",
    "2. Test reducing frequency loss to 0.0 at early steps and linearly increasing it. \n",
    "\n",
    "3. For upsampling replace transposed conv with nn.Upsample followed by Conv2d. add anti-alias filter in the downsampling path to avoid aliasing.\n",
    "\n",
    "---\n",
    "\n",
    "### Further references:\n",
    "\n",
    "**Dataset**: [DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K) \\\n",
    "**VGG Perceptual Loss**: [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155) \\\n",
    "**NeRF Positional Encodings**: [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934) \\\n",
    "**UNet**: [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "**Reproducibility**: Download the [DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K) dataset and create the folder **datasets/DIV2K** next to the notebook. Put **training HR** images into **datasets/DIV2K/train/HR/** and **validation HR** images into **datasets/DIV2K/valid/HR/**. The notebook expects PNG files in those folders. The training loop writes checkpoints under checkpoints/ and will resume automatically if it finds compatible files. The notebook has been developed with an **Nvidia RTX 5060 Ti with 16 GB VRAM** in mind for the reported timings and memory behavior. If you use a different GPU, reduce batch size and base channel width accordingly to avoid out of memory errors.\n",
    "\n",
    "To run the notebook, open the notebook and execute the cells in order. If you want a shorter test use the debug flag to reduce epochs and batch size. To restore a training run, place a checkpoint file named like **resshift_epoch_{N}.pt** in checkpoints/ then rerun the notebook; it will auto-detect the latest one and resume. For evaluation, run the provided evaluation cell which will generate comparison images showing LR (bicubic), SR, and HR and save them to the checkpoints folder.\n",
    "\n",
    "Project structure example:\n",
    "\n",
    "```\n",
    ". \\\n",
    "├── efficient_diffusion_super_res.ipynb                 # main notebook with training and evaluation \\\n",
    "├── efficient_diffusion_super_res_reformatted.py        # clean script version of notebook for headless runs \\\n",
    "├── checkpoints/                                        # saved checkpoints and eval images \\\n",
    "├── datasets/ \\\n",
    "│   └── DIV2K/ \\\n",
    "│       ├── train/HR/                                   # 800 HR images \\\n",
    "│       └── valid/HR/                                   # 200 HR images \\\n",
    "└── README.md \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
